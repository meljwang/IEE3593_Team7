{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0627b8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a30cee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path.cwd().parent   \n",
    "DATA_PATH = BASE_DIR / \"dataset\" / \"card_usage\" / \"card_subway_with_timeseries_features.csv\"\n",
    "OUT_DIR = BASE_DIR / \"ml_outputs\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATE_COL = \"date\"\n",
    "TARGET_COL = \"total_flow\"\n",
    "\n",
    "SEQ_LEN = 14\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f98d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train date range: 2015-01-01 00:00:00 -> 2023-10-20 00:00:00\n",
      "Val date range  : 2023-10-21 00:00:00 -> 2025-11-30 00:00:00\n",
      "Val stations total: 653 | unseen in train: 11\n",
      "Val rows after filtering unseen stations: 616355\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, parse_dates=[DATE_COL], low_memory=False)\n",
    "\n",
    "# Time split \n",
    "split_date = df[DATE_COL].quantile(0.8)\n",
    "train_df = df[df[DATE_COL] <= split_date].copy()\n",
    "val_df   = df[df[DATE_COL] >  split_date].copy()\n",
    "\n",
    "print(\"Train date range:\", train_df[DATE_COL].min(), \"->\", train_df[DATE_COL].max())\n",
    "print(\"Val date range  :\", val_df[DATE_COL].min(), \"->\", val_df[DATE_COL].max())\n",
    "\n",
    "# keep only stations seen in training\n",
    "train_stations = set(train_df[\"station_key\"].unique())\n",
    "val_stations = set(val_df[\"station_key\"].unique())\n",
    "unseen = sorted(list(val_stations - train_stations))\n",
    "print(f\"Val stations total: {len(val_stations)} | unseen in train: {len(unseen)}\")\n",
    "\n",
    "val_df = val_df[val_df[\"station_key\"].isin(train_stations)].copy()\n",
    "print(\"Val rows after filtering unseen stations:\", len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89c511cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Per-station normalisation \n",
    "# Compute mu/sigma on train dataset only\n",
    "stats = train_df.groupby(\"station_key\")[TARGET_COL].agg([\"mean\", \"std\"]).reset_index()\n",
    "stats[\"std\"] = stats[\"std\"].replace(0, np.nan)  # avoid div by zero\n",
    "stats = stats.fillna({\"std\": 1.0})\n",
    "\n",
    "train_df = train_df.merge(stats, on=\"station_key\", how=\"left\")\n",
    "val_df   = val_df.merge(stats, on=\"station_key\", how=\"left\")\n",
    "\n",
    "# Normalize target for stable training\n",
    "train_df[\"flow_norm\"] = (train_df[TARGET_COL] - train_df[\"mean\"]) / train_df[\"std\"]\n",
    "val_df[\"flow_norm\"]   = (val_df[TARGET_COL]   - val_df[\"mean\"])   / val_df[\"std\"]\n",
    "\n",
    "# Fill any missing station stats in val (rare if val contains new stations)\n",
    "val_df[\"mean\"] = val_df[\"mean\"].fillna(val_df[TARGET_COL].mean())\n",
    "val_df[\"std\"]  = val_df[\"std\"].fillna(val_df[TARGET_COL].std() if val_df[TARGET_COL].std() > 0 else 1.0)\n",
    "val_df[\"flow_norm\"] = (val_df[TARGET_COL] - val_df[\"mean\"]) / val_df[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e047f28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using features: ['is_weekend', 'day_of_week_num', 'day_of_month', 'week_of_year', 'flow_lag_1', 'flow_lag_7', 'flow_lag_14', 'flow_roll_mean_7', 'flow_roll_mean_14', 'flow_roll_std_7', 'flow_roll_std_14', 'flow_ratio', 'flow_diff', 'latitude', 'longitude']\n",
      "Train samples: 2477991\n",
      "Val samples  : 607448\n"
     ]
    }
   ],
   "source": [
    "# Build sequences per station\n",
    "# Dynamically select only features that exist in the DataFrame\n",
    "possible_features = [\n",
    "    \"flow_norm\", \"is_weekend\", \"day_of_week_num\", \"day_of_month\", \"week_of_year\",\n",
    "    \"flow_lag_1\", \"flow_lag_7\", \"flow_lag_14\", \"flow_roll_mean_7\", \"flow_roll_mean_14\",\n",
    "    \"flow_roll_std_7\", \"flow_roll_std_14\", \"flow_ratio\", \"flow_diff\"\n",
    "]\n",
    "if \"latitude\" in df.columns and \"longitude\" in df.columns:\n",
    "    possible_features += [\"latitude\", \"longitude\"]\n",
    "\n",
    "FEATURE_COLS = [col for col in possible_features if col in df.columns]\n",
    "print(\"Using features:\", FEATURE_COLS)\n",
    "\n",
    "class SubwaySeqDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Each item:\n",
    "      X: (SEQ_LEN, F) features for days t-SEQ_LEN+1..t\n",
    "      y: scalar flow_norm at day t+1\n",
    "      meta: station_key, date_of_y, mean, std (for denorm)\n",
    "    \"\"\"\n",
    "    def __init__(self, frame: pd.DataFrame, seq_len: int, feature_cols: list[str]):\n",
    "        self.seq_len = seq_len\n",
    "        self.feature_cols = feature_cols\n",
    "\n",
    "        self.samples = []\n",
    "        for st, g in frame.groupby(\"station_key\"):\n",
    "            g = g.sort_values(DATE_COL).reset_index(drop=True)\n",
    "            if len(g) < seq_len + 1:\n",
    "                continue\n",
    "\n",
    "            feats = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "            y = g[\"flow_norm\"].to_numpy(dtype=np.float32)\n",
    "            mu = g[\"mean\"].to_numpy(dtype=np.float32)\n",
    "            sd = g[\"std\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "            # keep date as plain string (safe for collate)\n",
    "            dates = g[DATE_COL].dt.strftime(\"%Y-%m-%d\").astype(str).to_numpy()\n",
    "\n",
    "            for t in range(seq_len - 1, len(g) - 1):\n",
    "                X = feats[t - (seq_len - 1): t + 1]\n",
    "                target = y[t + 1]\n",
    "                meta = {\n",
    "                    \"station_key\": str(st),\n",
    "                    \"date_y\": dates[t + 1],\n",
    "                    \"mean\": float(mu[t + 1]),\n",
    "                    \"std\": float(sd[t + 1]),\n",
    "                }\n",
    "                self.samples.append((X, target, meta))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X, y, meta = self.samples[idx]\n",
    "        return torch.from_numpy(X), torch.tensor(y, dtype=torch.float32), meta\n",
    "\n",
    "\n",
    "train_ds = SubwaySeqDataset(train_df, SEQ_LEN, FEATURE_COLS)\n",
    "val_ds   = SubwaySeqDataset(val_df,   SEQ_LEN, FEATURE_COLS)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples  :\", len(val_ds))\n",
    "\n",
    "def custom_collate(batch):\n",
    "    Xs, ys, metas = zip(*batch)\n",
    "    return torch.stack(Xs), torch.stack(ys), list(metas)\n",
    "\n",
    "# smaller batch helps avoid \"predict-mean\" collapse\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, collate_fn=custom_collate)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbb6d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Persistence baseline (val): (10885.0859375, 19490.745291034924)\n"
     ]
    }
   ],
   "source": [
    "# Model: LSTM\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, input_size: int, hidden_size: int = 128, num_layers: int = 2, dropout: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last).squeeze(-1)\n",
    "\n",
    "\n",
    "LR = 3e-4\n",
    "model = LSTMRegressor(input_size=len(FEATURE_COLS)).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def eval_loader(loader):\n",
    "    model.eval()\n",
    "    preds_norm, trues_norm, metas = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y, meta in loader:\n",
    "            X = X.to(DEVICE, dtype=torch.float32)\n",
    "            y = y.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "            yhat = model(X)\n",
    "\n",
    "            preds_norm.append(yhat.detach().cpu().numpy())\n",
    "            trues_norm.append(y.detach().cpu().numpy())\n",
    "\n",
    "        \n",
    "            if isinstance(meta, dict):\n",
    "                metas.append(meta)\n",
    "            else:\n",
    "                metas.extend(list(meta))\n",
    "\n",
    "    preds_norm = np.concatenate(preds_norm)\n",
    "    trues_norm = np.concatenate(trues_norm)\n",
    "\n",
    "    mu = np.array([m[\"mean\"] for m in metas], dtype=np.float32)\n",
    "    sd = np.array([m[\"std\"] for m in metas], dtype=np.float32)\n",
    "\n",
    "    preds = preds_norm * sd + mu\n",
    "    trues = trues_norm * sd + mu\n",
    "\n",
    "    mae = float(np.mean(np.abs(preds - trues)))\n",
    "    rmse = float(math.sqrt(np.mean((preds - trues) ** 2)))\n",
    "    return mae, rmse, preds, trues, metas\n",
    "\n",
    "# Sanity check: baseline \"predict tomorrow = today\"\n",
    "def eval_persistence(loader):\n",
    "    preds_norm, trues_norm, metas = [], [], []\n",
    "    for X, y, meta in loader:\n",
    "        pred = X[:, -1, 0]  # last timestep's flow_norm\n",
    "        preds_norm.append(pred.numpy())\n",
    "        trues_norm.append(y.numpy())\n",
    "        metas.extend(meta)\n",
    "\n",
    "    preds_norm = np.concatenate(preds_norm)\n",
    "    trues_norm = np.concatenate(trues_norm)\n",
    "\n",
    "    mu = np.array([m[\"mean\"] for m in metas], dtype=np.float32)\n",
    "    sd = np.array([m[\"std\"] for m in metas], dtype=np.float32)\n",
    "    preds = preds_norm * sd + mu\n",
    "    trues = trues_norm * sd + mu\n",
    "\n",
    "    mae = float(np.mean(np.abs(preds - trues)))\n",
    "    rmse = float(math.sqrt(np.mean((preds - trues) ** 2)))\n",
    "    return mae, rmse\n",
    "\n",
    "print(\"Persistence baseline (val):\", eval_persistence(val_loader))\n",
    "\n",
    "history = {\"train_loss\": [], \"val_mae\": [], \"val_rmse\": []}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "\n",
    "    for X, y, _ in train_loader:\n",
    "        X = X.to(DEVICE, dtype=torch.float32)\n",
    "        y = y.to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # helps stabilize training / avoid collapse\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        running += float(loss.item())\n",
    "\n",
    "    train_loss = running / max(1, len(train_loader))\n",
    "    val_mae, val_rmse, _, _, _ = eval_loader(val_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    history[\"val_mae\"].append(val_mae)\n",
    "    history[\"val_rmse\"].append(val_rmse)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | train_loss={train_loss:.5f} | val_MAE={val_mae:,.2f} | val_RMSE={val_rmse:,.2f}\")\n",
    "\n",
    "val_mae, val_rmse, preds, trues, metas = eval_loader(val_loader)\n",
    "print(\"\\nFinal Validation\")\n",
    "print(f\"MAE  : {val_mae:,.2f}\")\n",
    "print(f\"RMSE : {val_rmse:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddc510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved outputs to: /Users/melvinang/Documents/NUS/Y4 Winter/Team7/IEE3593_Team7/ml_outputs\n",
      "- lstm_training_loss.png\n",
      "- lstm_pred_vs_actual_scatter.png\n",
      "- lstm_station_timeseries.png\n",
      "- lstm_val_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Training curve\n",
    "plt.figure()\n",
    "plt.plot(history[\"train_loss\"], label=\"train MSE loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"LSTM Training Loss\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"lstm_training_loss.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Scatterplot: predicted vs actual\n",
    "plt.figure()\n",
    "# sample to make plot lighter if huge\n",
    "idx = np.random.choice(len(preds), size=min(30000, len(preds)), replace=False)\n",
    "plt.scatter(trues[idx], preds[idx], s=3)\n",
    "minv = min(trues[idx].min(), preds[idx].min())\n",
    "maxv = max(trues[idx].max(), preds[idx].max())\n",
    "plt.plot([minv, maxv], [minv, maxv])\n",
    "plt.xlabel(\"Actual next-day total_flow\")\n",
    "plt.ylabel(\"Predicted next-day total_flow\")\n",
    "plt.title(\"Predicted vs Actual (Validation)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"lstm_pred_vs_actual_scatter.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Time series plot for one station (top-busy station in val)\n",
    "meta_df = pd.DataFrame({\n",
    "    \"station_key\": [m[\"station_key\"] for m in metas],\n",
    "    \"date_y\": pd.to_datetime([m[\"date_y\"] for m in metas]),\n",
    "    \"pred\": preds,\n",
    "    \"true\": trues,\n",
    "})\n",
    "# pick station with most rows\n",
    "pick_station = meta_df[\"station_key\"].value_counts().index[0]\n",
    "st_df = meta_df[meta_df[\"station_key\"] == pick_station].sort_values(\"date_y\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(st_df[\"date_y\"], st_df[\"true\"], label=\"Actual\")\n",
    "plt.plot(st_df[\"date_y\"], st_df[\"pred\"], label=\"Predicted\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Total Flow\")\n",
    "plt.title(f\"Next-day Forecast for Station {pick_station}\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"lstm_station_timeseries.png\", dpi=200)\n",
    "plt.close()\n",
    "\n",
    "# Save a small CSV of predictions for reporting\n",
    "meta_df.to_csv(OUT_DIR / \"lstm_val_predictions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"\\nSaved outputs to:\", OUT_DIR)\n",
    "print(\"- lstm_training_loss.png\")\n",
    "print(\"- lstm_pred_vs_actual_scatter.png\")\n",
    "print(\"- lstm_station_timeseries.png\")\n",
    "print(\"- lstm_val_predictions.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
